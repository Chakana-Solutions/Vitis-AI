<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
<!-- OneTrust Cookies Consent Notice start for xilinx.github.io -->

<script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="03af8d57-0a04-47a6-8f10-322fa00d8fc7" ></script>
<script type="text/javascript">
function OptanonWrapper() { }
</script>
<!-- OneTrust Cookies Consent Notice end for xilinx.github.io -->
<!-- Google Tag Manager -->
<script type="text/plain" class="optanon-category-C0002">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-5RHQV7');</script>
<!-- End Google Tag Manager -->
  <title>Quick Start Guide for Zynq™ UltraScale+™ &mdash; Vitis™ AI 3.0 documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="First Steps" href="../workflow.html" />
    <link rel="prev" title="Quick Start Guide for VCK190" href="vck190.html" /> 
</head>

<body class="wy-body-for-nav">

<!-- Google Tag Manager -->
<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-5RHQV7" height="0" width="0" style="display:none;visibility:hidden" class="optanon-category-C0002"></iframe></noscript>
<!-- End Google Tag Manager --> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
            <a href="../../index.html" class="icon icon-home"> Vitis™ AI
            <img src="../../_static/xilinx-header-logo.svg" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                3.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Setup and Install</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../reference/release_notes_3.0.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/system_requirements.html">System Requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/install.html">Host Install Instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="vck5000.html">Quick Start Guide for VCK5000</a></li>
<li class="toctree-l1"><a class="reference internal" href="vck190.html">Quick Start Guide for VCK190</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quick Start Guide for MPSOC</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#prerequisites">Prerequisites</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#host-requirements">Host Requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="#applicable-targets">Applicable Targets</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#quickstart">Quickstart</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#clone-the-vitis-ai-repository">Clone the Vitis AI Repository</a></li>
<li class="toctree-l3"><a class="reference internal" href="#install-docker">Install Docker</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pull-vitis-ai-docker">Pull Vitis AI Docker</a></li>
<li class="toctree-l3"><a class="reference internal" href="#setup-the-host">Setup the Host</a></li>
<li class="toctree-l3"><a class="reference internal" href="#setup-the-target">Setup the Target</a></li>
<li class="toctree-l3"><a class="reference internal" href="#run-the-vitis-ai-examples">Run the Vitis AI Examples</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#pytorch-tutorial">PyTorch Tutorial</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#quantizing-the-model">Quantizing the Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compile-the-model">Compile the model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-deployment">Model Deployment</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Workflow and Components</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../workflow.html">First Steps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workflow.html#supported-evaluation-targets">Supported Evaluation Targets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workflow-dpu.html">What is a DPU?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workflow-model-zoo.html">Vitis AI Model Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workflow-model-development.html">Developing a Model for Vitis AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workflow-system-integration.html">Integrating the DPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workflow-model-deployment.html">Deploying a Model with Vitis AI</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Additional Information</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/release_documentation.html">Vitis AI User Guides &amp; IP Product Guides</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Xilinx/Vitis-AI-Tutorials">Vitis AI Developer Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workflow-third-party.html">Third-party Inference Stack Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/version_compatibility.html">IP and Tools Compatibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/branching_tagging_strategy.html">Branching and Tagging Strategy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources and Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/additional_resources.html">Technical Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/additional_resources.html#additional-resources">Additional Resources</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Related AMD Solutions</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Xilinx/DPU-PYNQ">DPU-PYNQ</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/finn/">FINN &amp; Brevitas</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/inference-server/">Inference Server</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/amd/UIF">Unified Inference Frontend</a></li>
<li class="toctree-l1"><a class="reference external" href="https://onnxruntime.ai/docs/execution-providers/community-maintained/Vitis-AI-ExecutionProvider.html">Vitis AI ONNX Runtime Execution Provider</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/VVAS/">Vitis Video Analytics SDK</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: black" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Vitis™ AI</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Quick Start Guide for Zynq™ UltraScale+™</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/docs/quickstart/mpsoc.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="quick-start-guide-for-zynq-trade-ultrascale-trade">
<h1>Quick Start Guide for Zynq™ UltraScale+™<a class="headerlink" href="#quick-start-guide-for-zynq-trade-ultrascale-trade" title="Permalink to this heading">¶</a></h1>
<p>The AMD <strong>DPUCZDX8G</strong> for Zynq™ Ultrascale+™ is a configurable computation engine dedicated to convolutional neural networks. It supports a highly optimized instruction set, enabling the deployment of most convolutional neural networks. The following instructions will help you to install the software and packages required to support KV260/ZCU102/ZCU104.</p>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this heading">¶</a></h2>
<section id="host-requirements">
<h3>Host Requirements<a class="headerlink" href="#host-requirements" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>Confirm that your development machine meets the minimum <a class="reference internal" href="../reference/system_requirements.html"><span class="doc">Host System Requirements</span></a>.</p></li>
<li><p>Confirm that you have at least <strong>100GB</strong> of free space in the target partition.</p></li>
</ul>
</section>
<section id="applicable-targets">
<h3>Applicable Targets<a class="headerlink" href="#applicable-targets" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>This quickstart is applicable to the following hardware platforms:</p></li>
</ul>
<table class="docutils align-default">
<colgroup>
<col style="width: 60%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Product</p></th>
<th class="head"><p>Supported Target(s)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Zynq™ Ultrascale+™ Adaptable SoC Evaluation Boards</p></td>
<td><p><a class="reference external" href="https://www.xilinx.com/zcu102">ZCU102</a> / <a class="reference external" href="https://www.xilinx.com/zcu104">ZCU104</a> / <a class="reference external" href="https://www.xilinx.com/kria">Kria KV260</a></p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="quickstart">
<h2>Quickstart<a class="headerlink" href="#quickstart" title="Permalink to this heading">¶</a></h2>
<section id="clone-the-vitis-ai-repository">
<h3>Clone the Vitis AI Repository<a class="headerlink" href="#clone-the-vitis-ai-repository" title="Permalink to this heading">¶</a></h3>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Host<span class="o">]</span> $ git clone https://github.com/Xilinx/Vitis-AI
<span class="o">[</span>Host<span class="o">]</span> $ <span class="nb">cd</span> Vitis-AI
</pre></div>
</div>
</section>
<section id="install-docker">
<h3>Install Docker<a class="headerlink" href="#install-docker" title="Permalink to this heading">¶</a></h3>
<p>Make sure that the Docker engine is installed according to the official Docker <a class="reference external" href="https://docs.docker.com/engine/install/">documentation</a>.</p>
<p>The Docker daemon always runs as the root user. Non-root users must be <a class="reference external" href="https://docs.docker.com/engine/install/linux-postinstall/">added</a> to the docker group. Do this now.</p>
<p>Perform a quick and simple test of your Docker installation by executing the following command.  This command will download a test image from Docker Hub and run it in a container. When the container runs successfully, it prints a “Hello World” message and exits.</p>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Host<span class="o">]</span> $ docker run hello-world
</pre></div>
</div>
<p>Finally, verify that the version of Docker that you have installed meets the minimum <a class="reference internal" href="../reference/system_requirements.html"><span class="doc">Host System Requirements</span></a> by running the following command</p>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Host<span class="o">]</span> $ docker --version
</pre></div>
</div>
</section>
<section id="pull-vitis-ai-docker">
<h3>Pull Vitis AI Docker<a class="headerlink" href="#pull-vitis-ai-docker" title="Permalink to this heading">¶</a></h3>
<p>For this quickstart tutorial we will simply use the pre-built Vitis AI PyTorch CPU Docker.  It is generic, does not require the user to build the container, and has no specific GPU enablement requirements.  More advanced users can optionally skip this step and jump to the <a class="reference internal" href="../install/install.html"><span class="doc">Full Install Instructions</span></a> but we would recommend that new users start with this simpler first step.</p>
<p>Pull and start the latest Vitis AI Docker using the following commands:</p>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Host<span class="o">]</span> $ docker pull xilinx/vitis-ai-pytorch-cpu:latest
<span class="o">[</span>Host<span class="o">]</span> $ ./docker_run.sh xilinx/vitis-ai-pytorch-cpu:latest
</pre></div>
</div>
</section>
<section id="setup-the-host">
<h3>Setup the Host<a class="headerlink" href="#setup-the-host" title="Permalink to this heading">¶</a></h3>
<p>It will be useful to you later on to have the cross-compiler installed.  This will allow you to compile MPSoC applications on your host machine inside Docker.  Run the following commands to install the cross-compilation environment:</p>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Host<span class="o">]</span> $ <span class="nb">cd</span> Vitis-AI/board_setup/mpsoc
<span class="o">[</span>Host<span class="o">]</span> $ ./host_cross_compiler_setup.sh
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Perform these steps on your local host Linux operating system (not inside the docker container). By default, the cross compiler will be installed in <code class="docutils literal notranslate"><span class="pre">~/petalinux_sdk_2022.2</span></code>.  The ~/petalinux_sdk_2022.2 path is recommended for the installation. Regardless of the path you choose for the installation, make sure the path has read-write permissions. In this quickstart, it is installed in ~/petalinux_sdk_2022.2</p>
</div>
<p>When the installation is complete, follow the prompts and execute the following command:</p>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Host<span class="o">]</span> $ <span class="nb">source</span> ~/petalinux_sdk_2022.2/environment-setup-cortexa72-cortexa53-xilinx-linux
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you close the current terminal, you must re-execute the above instructions in the new terminal interface.</p>
</div>
<p>Cross compile the sample taking resnet50 as an example:</p>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Docker<span class="o">]</span> $ <span class="nb">cd</span> Vitis-AI/examples/vai_runtime/resnet50
<span class="o">[</span>Docker<span class="o">]</span> $ bash –x build.sh
</pre></div>
</div>
<p>If the compilation process does not report any error and the executable file resnet50 is generated, then the host environment is installed correctly.</p>
</section>
<section id="setup-the-target">
<h3>Setup the Target<a class="headerlink" href="#setup-the-target" title="Permalink to this heading">¶</a></h3>
<p>The Vitis AI Runtime packages, VART samples, Vitis-AI-Library samples, and models are built into the board image, enhancing the user experience. Therefore, the user need not install Vitis AI Runtime packages and model packages on the board separately.</p>
<ol class="arabic simple">
<li><p>Make the target / host connections as shown in the images below.  Plug in the power adapter, ethernet cable, and a DisplayPort monitor (optional) and connect the USB-UART interface to the host.  If one is available, connect a USB webcam to the target.</p></li>
</ol>
<a class="reference internal image-reference" href="../../_images/kria_setup.png"><img alt="../../_images/kria_setup.png" src="../../_images/kria_setup.png" style="width: 1300px;" /></a>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Kria KV260 can be leveraged with either HDMI or DisplayPort monitors.</p>
</div>
<ol class="arabic simple" start="2">
<li><p>Download the SD card image from the appropriate link:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.xilinx.com/member/forms/download/design-license-xef.html?filename=xilinx-zcu102-dpu-v2022.2-v3.0.0.img.gz">ZCU102</a></p></li>
<li><p><a class="reference external" href="https://www.xilinx.com/member/forms/download/design-license-xef.html?filename=xilinx-zcu104-dpu-v2022.2-v3.0.0.img.gz">ZCU104</a></p></li>
<li><p><a class="reference external" href="https://www.xilinx.com/member/forms/download/design-license-xef.html?filename=xilinx-kv260-dpu-v2022.2-v3.0.0.img.gz">KV260</a></p></li>
</ul>
</li>
<li><p>Use BalenaEtcher to burn the image file onto the SD card.</p></li>
</ol>
<a class="reference internal image-reference" href="../../_images/Etcher.png"><img alt="../../_images/Etcher.png" src="../../_images/Etcher.png" style="width: 1300px;" /></a>
<ol class="arabic" start="4">
<li><p>Insert the imaged SD card into the target board.</p></li>
<li><p>Log in to the board with your serial terminal application of choice, using the parameters listed below.</p>
<blockquote>
<div><ul class="simple">
<li><p>Baud Rate: 115200</p></li>
<li><p>Data Bit: 8</p></li>
<li><p>Stop Bit: 1</p></li>
<li><p>No Parity</p></li>
</ul>
</div></blockquote>
</li>
<li><p>The IP address for the target can be found with the command below.</p></li>
</ol>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Target<span class="o">]</span> $ ifconfig
</pre></div>
</div>
<a class="reference internal image-reference" href="../../_images/ifconfig.png"><img alt="../../_images/ifconfig.png" src="../../_images/ifconfig.png" style="width: 1300px;" /></a>
<p>If you are using a point-to-point connection or DHCP is not available, you can manually set the IP address:</p>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Target<span class="o">]</span> $ ifconfig eth0 <span class="o">[</span>target_ip_address<span class="o">]</span>
</pre></div>
</div>
<ol class="arabic simple" start="7">
<li><p>Next, connect to the board via SSH.  The password is ‘root’</p></li>
</ol>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Host<span class="o">]</span> $ ssh -X root@<span class="o">[</span>target_ip_address<span class="o">]</span>
</pre></div>
</div>
<ol class="arabic simple" start="8">
<li><p>If you have not connected a DisplayPort monitor, it is recommended that you export the display.  If you do not do so, the examples will not run as expected.</p></li>
</ol>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Target<span class="o">]</span> $ <span class="nb">export</span> <span class="nv">DISPLAY</span><span class="o">=</span>:0.0
</pre></div>
</div>
<ol class="arabic simple" start="9">
<li><p>Download the model.</p></li>
</ol>
<p>You can now select a model from the Vitis AI Model Zoo <a class="reference external" href="../workflow-model-zoo.html">Vitis AI Model Zoo</a>.  Navigate to the  <a class="reference external" href="https://github.com/Xilinx/Vitis-AI/tree/master/model_zoo/model-list">model-list subdirectory</a>  and select the model that you wish to test. For each model, a YAML file provides key details of the model. In the YAML file there are separate hyperlinks to download the model for each supported target.  Choose the correct link for your target platform and download the model.</p>
<blockquote>
<div><ol class="loweralpha simple">
<li><p>Take the ResNet50 model as an example.</p></li>
</ol>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Host<span class="o">]</span> $ <span class="nb">cd</span> /workspace
<span class="o">[</span>Host<span class="o">]</span> $ wget https://www.xilinx.com/bin/public/openDownload?filename<span class="o">=</span>resnet50-zcu102_zcu104_kv260-r3.0.0.tar.gz -O resnet50-zcu102_zcu104_kv260-r3.0.0.tar.gz
</pre></div>
</div>
<ol class="loweralpha simple" start="2">
<li><p>Copy the downloaded file to the board using scp with the following command:</p></li>
</ol>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Host<span class="o">]</span> $ scp resnet50-zcu102_zcu104_kv260-r3.0.0.tar.gz root@IP_OF_BOARD:~/
</pre></div>
</div>
<ol class="loweralpha simple" start="3">
<li><p>Install the model package:</p></li>
</ol>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Target<span class="o">]</span> $ tar -xzvf resnet50-zcu102_zcu104_kv260-r3.0.0.tar.gz
<span class="o">[</span>Target<span class="o">]</span> $ cp resnet50 /usr/share/vitis_ai_library/models -r
</pre></div>
</div>
</div></blockquote>
</section>
<section id="run-the-vitis-ai-examples">
<span id="mpsoc-run-vitis-ai-examples"></span><h3>Run the Vitis AI Examples<a class="headerlink" href="#run-the-vitis-ai-examples" title="Permalink to this heading">¶</a></h3>
<ol class="arabic simple">
<li><p>Download <a class="reference external" href="https://www.xilinx.com/bin/public/openDownload?filename=vitis_ai_runtime_r3.0.0_image_video.tar.gz">vitis_ai_runtime_r3.0.0_image_video.tar.gz</a> to your host, and copy the file to the the target using scp:</p></li>
</ol>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Host<span class="o">]</span> $ scp vitis_ai_runtime_r3.0.*_image_video.tar.gz root@<span class="o">[</span>IP_OF_BOARD<span class="o">]</span>:~/
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Unzip the <code class="docutils literal notranslate"><span class="pre">vitis_ai_runtime_r3.0.0_image_video.tar.gz</span></code> package on the target.</p></li>
</ol>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Target<span class="o">]</span> $ <span class="nb">cd</span> ~
<span class="o">[</span>Target<span class="o">]</span> $ tar -xzvf vitis_ai_runtime_r*3.0._image_video.tar.gz -C Vitis-AI/examples/vai_runtime
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Navigate to the example directory on the target board. Take <code class="docutils literal notranslate"><span class="pre">resnet50</span></code> as an example.</p></li>
</ol>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Target<span class="o">]</span> $ <span class="nb">cd</span> ~/Vitis-AI/examples/vai_runtime/resnet50
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>Run the example.</p></li>
</ol>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Target<span class="o">]</span> $ ./resnet50 /usr/share/vitis_ai_library/models/resnet50/resnet50.xmodel
</pre></div>
</div>
<ol class="arabic simple" start="5">
<li><p>An image should appear on the display connected to the target and the console should reflect the following output:</p></li>
</ol>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span>Image : <span class="m">001</span>.jpg
top<span class="o">[</span><span class="m">0</span><span class="o">]</span> <span class="nv">prob</span> <span class="o">=</span> <span class="m">0</span>.982662  <span class="nv">name</span> <span class="o">=</span> brain coral
top<span class="o">[</span><span class="m">1</span><span class="o">]</span> <span class="nv">prob</span> <span class="o">=</span> <span class="m">0</span>.008502  <span class="nv">name</span> <span class="o">=</span> coral reef
top<span class="o">[</span><span class="m">2</span><span class="o">]</span> <span class="nv">prob</span> <span class="o">=</span> <span class="m">0</span>.006621  <span class="nv">name</span> <span class="o">=</span> jackfruit, jak, jack
top<span class="o">[</span><span class="m">3</span><span class="o">]</span> <span class="nv">prob</span> <span class="o">=</span> <span class="m">0</span>.000543  <span class="nv">name</span> <span class="o">=</span> puffer, pufferfish, blowfish, globefish
top<span class="o">[</span><span class="m">4</span><span class="o">]</span> <span class="nv">prob</span> <span class="o">=</span> <span class="m">0</span>.000330  <span class="nv">name</span> <span class="o">=</span> eel
</pre></div>
</div>
<p>These results reflect the classification of a single test image located in the  <code class="docutils literal notranslate"><span class="pre">~/Vitis-AI/examples/vai_library/images</span></code>      directory.</p>
</section>
</section>
<section id="pytorch-tutorial">
<h2>PyTorch Tutorial<a class="headerlink" href="#pytorch-tutorial" title="Permalink to this heading">¶</a></h2>
<p>This tutorial assumes that Vitis AI has been installed and that the MPSOC board has been configured, as explained in the installation instructions above. For additional information on the Vitis-AI Quantizer, Optimizer, or Compiler, please refer to the Vitis AI User Guide.</p>
<section id="quantizing-the-model">
<h3>Quantizing the Model<a class="headerlink" href="#quantizing-the-model" title="Permalink to this heading">¶</a></h3>
<p>Quantization reduces the precision of network weights and activations to optimize memory usage and computational efficiency while maintaining acceptable levels of accuracy. Inference is computationally expensive and requires high memory bandwidths to satisfy the
low-latency and high-throughput requirements of Edge applications. Quantization and channel pruning techniques are employed to address these issues while achieving high performance and high energy efficiency with little degradation in accuracy. The Vitis AI Quantizer takes a
floating-point model as an input and performs pre-processing (folds batchnorms and removes nodes not required for inference), and finally quantizes the weights/biases and activations to the given bit width.</p>
<ol class="arabic simple">
<li><p>In the Vitis AI PyTorch docker container, create a new workspace to store the test dataset, models, and python scripts required for quantization.</p></li>
</ol>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Docker<span class="o">]</span> $ mkdir -p resnet18/model
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Download the pre-trained resnet18 model from PyTorch to the docker environment and store it in the  <code class="docutils literal notranslate"><span class="pre">model</span></code>  folder . This is the floating point (FP32) model that will be quantized to INT8 precision for deployment on the target.</p></li>
</ol>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Docker<span class="o">]</span> $ <span class="nb">cd</span> /workspace/resnet18/model
<span class="o">[</span>Docker<span class="o">]</span> $ wget https://download.pytorch.org/models/resnet18-5c106cde.pth -O resnet18.pth
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <a class="reference external" href="../workflow-model-zoo.html">Vitis AI Model Zoo</a> also provides optimized deep learning models to speed up the deployment of deep learning inference on adaptable AMD platforms. For this tutorial we have chosen to use an open-source PyTorch model to showcase that models from the community can also be deployed.</p>
</div>
<ol class="arabic simple" start="3">
<li><p>Copy the resnet18 quantization script to your workspace. This script contains the Quantizer API calls that will be executed in order to quantize the model.</p></li>
</ol>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Docker<span class="o">]</span> $ cp src/vai_quantizer/vai_q_pytorch/example/resnet18_quant.py resnet18/
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>Click the link to download the <a class="reference external" href="https://www.kaggle.com/datasets/ifigotin/imagenetmini-1000/download?datasetVersionNumber=1">ImageNet 1000 (mini)</a> dataset from Kaggle. Move the zip file into your workspace and unzip the dataset.</p></li>
</ol>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Docker<span class="o">]</span> $ unzip archive.zip
</pre></div>
</div>
<ul class="simple">
<li><p>Your workspace directory should reflect the following:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>├── model
│   └──_resnet18.pth           # ResNet18 floating point model downloaded from PyTorch.
    │
├── imagenet-mini
│   ├── train                    # Training images folder. Will not be used in this tutorial.
│   │   └───  n01440764          # Class folders to group images.
│   └── val                      # Validation images that will be used for quantization and evaluation of the floating point model.
│       └───  n01440764
│
└── resnet18_quant.py            # Quantization python script.
</pre></div>
</div>
<ol class="arabic simple" start="5">
<li><p>Modify the default file paths for <code class="docutils literal notranslate"><span class="pre">data_dir</span></code> and <code class="docutils literal notranslate"><span class="pre">model_dir</span></code> in <code class="docutils literal notranslate"><span class="pre">resnet18_quant.py</span></code> to point to the validation images and floating point model.</p></li>
</ol>
<a class="reference internal image-reference" href="../../_images/quantize_p1.png"><img alt="../../_images/quantize_p1.png" src="../../_images/quantize_p1.png" style="width: 1300px;" /></a>
<ol class="arabic simple" start="6">
<li><p>Run the command below to start quantization. Generally, 100-1000 images are required for quantization and the number of iterations can be controlled through the the <code class="docutils literal notranslate"><span class="pre">subset_len</span></code> data loading argument. In this case, 200 images are forward propagated through the network for quantization.</p></li>
</ol>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Docker<span class="o">]</span> $ python resnet18_quant.py --quant_mode calib --subset_len <span class="m">200</span>
</pre></div>
</div>
<p>If the command runs successfully, the output directory <code class="docutils literal notranslate"><span class="pre">quantize_result</span></code> will be generated, containing two important files:</p>
<blockquote>
<div><dl class="simple">
<dt>-<code class="docutils literal notranslate"><span class="pre">ResNet.py</span></code></dt><dd><p>The converted vai_q_pytorch format model.</p>
</dd>
<dt>-<code class="docutils literal notranslate"><span class="pre">Quant_info.json</span></code></dt><dd><p>Quantization steps of tensors. Retain this file for evaluating quantized models.</p>
</dd>
</dl>
</div></blockquote>
<ol class="arabic simple" start="7">
<li><p>To evaluate the quantized model and generate the reference result, run the following command:</p></li>
</ol>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Docker<span class="o">]</span> $ python resnet18_quant.py --quant_mode <span class="nb">test</span>
</pre></div>
</div>
<ol class="arabic simple" start="8">
<li><p>To generate the quantized <code class="docutils literal notranslate"><span class="pre">.xmodel</span></code> file that will subsequently be compiled for the DPU, run the following command with <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> and <code class="docutils literal notranslate"><span class="pre">subset_len</span></code> arguments set to 1 to avoid redundant iterations.</p></li>
</ol>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Docker<span class="o">]</span> $ python resnet18_quant.py --quant_mode <span class="nb">test</span> --subset_len <span class="m">1</span> --batch_size<span class="o">=</span><span class="m">1</span> --deploy
</pre></div>
</div>
</section>
<section id="compile-the-model">
<h3>Compile the model<a class="headerlink" href="#compile-the-model" title="Permalink to this heading">¶</a></h3>
<p>The Vitis AI Compiler compiles the graph operators as a set of micro-coded instructions that are executed by the DPU.  In this step, we will compile the ResNet18 model that we quantized in the previous step.</p>
<ol class="arabic simple">
<li><p>The compiler takes the quantized <code class="docutils literal notranslate"><span class="pre">INT8.xmodel</span></code> and generates the deployable <code class="docutils literal notranslate"><span class="pre">DPU.xmodel</span></code> by running the command below.  Note that you must modify the command to specify the appropriate <code class="docutils literal notranslate"><span class="pre">arch.json</span></code> file for your target.</p></li>
</ol>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Docker<span class="o">]</span> $ <span class="nb">cd</span> /workspace/resnet18
<span class="o">[</span>Docker<span class="o">]</span> $ vai_c_xir -x quantize_result/ResNet_int.xmodel -a /opt/vitis_ai/compiler/arch/DPUCZDX8G/&lt;Target<span class="o">(</span>ex:KV260<span class="o">)</span>&gt;/arch.json -o resnet18_pt -n resnet18_pt
</pre></div>
</div>
<ul class="simple">
<li><p>If compilation is successful, the <code class="docutils literal notranslate"><span class="pre">resnet18_pt.xmodel</span></code> file should be generated according to the specified DPU architecture.</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p>Create a new file with your text editor of choice and name the file <code class="docutils literal notranslate"><span class="pre">resnet18_pt.prototxt</span></code>. Copy and paste the following lines of code:</p></li>
</ol>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span>model <span class="o">{</span>
   name : <span class="s2">&quot;resnet18_pt&quot;</span>
   kernel <span class="o">{</span>
         name: <span class="s2">&quot;resnet18_pt_0&quot;</span>
         mean: <span class="m">103</span>.53
         mean: <span class="m">116</span>.28
         mean: <span class="m">123</span>.675
         scale: <span class="m">0</span>.017429
         scale: <span class="m">0</span>.017507
         scale: <span class="m">0</span>.01712475
   <span class="o">}</span>
   model_type : CLASSIFICATION
   classification_param <span class="o">{</span>
          top_k : <span class="m">5</span>
          test_accuracy : <span class="nb">false</span>
          preprocess_type : VGG_PREPROCESS
   <span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">.prototxt</span></code> file is a Vitis AI™ Library configuration file that facilitates the uniform configuration management of model parameters. Please refer to the Vitis AI User Guide to learn more.</p></li>
<li><p>We can now deploy the quantized and compiled model on the MPSOC target.</p></li>
</ul>
</section>
<section id="model-deployment">
<h3>Model Deployment<a class="headerlink" href="#model-deployment" title="Permalink to this heading">¶</a></h3>
<ol class="arabic simple">
<li><p>Download the <code class="docutils literal notranslate"><span class="pre">resnet18_pt</span></code> folder from host to target using scp with the following command:</p></li>
</ol>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Docker<span class="o">]</span> $ scp -r resnet18_pt root@<span class="o">[</span>IP_OF_BOARD<span class="o">]</span>:/usr/share/vitis_ai_library/models/
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>The model will be located under the <code class="docutils literal notranslate"><span class="pre">/usr/share/vitis_ai_library/models/</span></code> folder along with the other Viitis-AI model examples.</p></li>
<li><p>Users can run real time inference using a USB web camera connected to the target with the command below:</p></li>
</ol>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Target<span class="o">]</span> $ <span class="nb">cd</span> Vitis-AI/examples/vai_library/samples/classification
<span class="o">[</span>Target<span class="o">]</span> $ ./test_video_classification resnet18_pt <span class="m">0</span> -t <span class="m">8</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">0</span></code> corresponds to the first USB camera device node. If you have multiple USB cameras, the value is 1,2,3, etc.  <code class="docutils literal notranslate"><span class="pre">-t</span></code> corresponds to the number of threads.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We recommend the Logitech BRIO for use with Vitis AI pre-built images.  The Logitech BRIO is capable of streaming RAW video at higher resolutions than most low-cost webcams.  When leveraging other low-cost webcams with the Vitis AI pre-built image, encoded video streams are actually decoded on the target’s ARM APU which can reduce inference performance. If a video stream does not appear on the connected display, reboot the board and run the command again.</p>
</div>
<ol class="arabic simple" start="4">
<li><p>The output should be as follows:</p></li>
</ol>
<ul class="simple">
<li><p>Congratulations! You have successfully quantized, compiled, and deployed a pre-trained model onto the MPSOC target.</p></li>
</ul>
</section>
</section>
</section>


           </div>
          </div>
          
				  
				  <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="vck190.html" class="btn btn-neutral float-left" title="Quick Start Guide for VCK190" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../workflow.html" class="btn btn-neutral float-right" title="First Steps" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022-2023, Advanced Micro Devices, Inc.
      <span class="lastupdated">Last updated on May 18, 2023.
      </span></p>
  </div>



										<div class="aem-Grid aem-Grid--16">
											<div class="aem-GridColumn aem-GridColumn--xxxlarge--none aem-GridColumn--xsmall--16 aem-GridColumn--offset--xsmall--0 aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--xsmall--none aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
												<div class="container-fluid sub-footer">

													                    <div class="row">
                        <div class="col-xs-24">
                          <p><a target="_blank" href="https://www.amd.com/en/corporate/copyright">Terms and Conditions</a> | <a target="_blank" href="https://www.amd.com/en/corporate/privacy">Privacy</a> | <a target="_blank" href="https://www.amd.com/en/corporate/cookies">Cookie Policy</a> | <a target="_blank" href="https://www.amd.com/en/corporate/trademarks">Trademarks</a> | <a target="_blank" href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf">Statement on Forced Labor</a> | <a target="_blank" href="https://www.amd.com/en/corporate/competition">Fair and Open Competition</a> | <a target="_blank" href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf">UK Tax Strategy</a> | <a target="_blank" href="https://docs.xilinx.com/v/u/9x6YvZKuWyhJId7y7RQQKA">Inclusive Terminology</a> | <a href="#cookiessettings" class="ot-sdk-show-settings">Cookies Settings</a></p>
                        </div>
                    </div>
												</div>
											</div>
										</div>
										
</br>


  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>


</body>
</html>